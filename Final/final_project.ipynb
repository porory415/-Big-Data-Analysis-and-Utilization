{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3839fe",
   "metadata": {},
   "source": [
    "# ë¹…ë°ì´í„°ë¶„ì„ê³¼í™œìš© ê¸°ë§í”„ë¡œì íŠ¸\n",
    "##### (ë°•í˜œë¹ˆ, ìœ ì§€ì›, ìœ¤ì¡°í˜„)\n",
    "\n",
    "## ì£¼ì œ:\n",
    "\n",
    "0. ê°œìš”\n",
    "1. Library Import\n",
    "#### -classification-\n",
    "2. ë°ì´í„° ì—…ë¡œë“œ\n",
    "3. EDA\n",
    "4. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "- 4-1. stemmer\n",
    "- 4.2 Bag of Words : ë¬¸ì¥ë³„ ë‹¨ì–´ ë²¡í„° ë§Œë“¤ê¸°\n",
    "- 4.3 Train : Test ë¡œ ë‚˜ëˆ„ê¸°\n",
    "5. ëª¨ë¸ë§\n",
    "- 5.1 Naive Baye's Classifier(MultinomialNB)\n",
    "- 5.2 Random Forest Classifier\n",
    "- 5.3 Support Vector Machine\n",
    "#### -regression-\n",
    "6. ë°ì´í„° íŒŒì•… (EDA: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)\n",
    "- 6-1. ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ íŒŒì•…\n",
    "- 6-2. ì¢…ì† ë³€ìˆ˜(ëª©í‘œ ë³€ìˆ˜) íƒìƒ‰\n",
    "- 6-3. ì„¤ëª… ë³€ìˆ˜ íƒìƒ‰\n",
    "- 6-4. ì„¤ëª…ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ íƒìƒ‰\n",
    "7. ëª¨ë¸ë§: íšŒê·€ ë¶„ì„\n",
    "- 7-1. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "- 7-2. íšŒê·€ ëª¨ë¸ë§\n",
    "- 7-3. ëª¨ë¸ í•´ì„\n",
    "- 7-4. ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë° ì„±ëŠ¥ í‰ê°€\n",
    "8. validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a97250",
   "metadata": {},
   "source": [
    "### 1. Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1270233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83d8128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a3149",
   "metadata": {},
   "source": [
    "# textblob ì˜¤ë¥˜ No module named 'textblob' ë‹¤ìš´ê²½ë¡œ ë¬¸ì œê°™ì€ë°.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f34238e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-beb514e823a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acc392",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d58515",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### 2. Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6265e704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Bitcoin_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebb2c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1751675, 13)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31017d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeSota Wilson</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>Biz Consultant, real estate, fintech, startups...</td>\n",
       "      <td>2009-04-26 20:05:09</td>\n",
       "      <td>8534.0</td>\n",
       "      <td>7605</td>\n",
       "      <td>4838</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:59:04</td>\n",
       "      <td>Blue Ridge Bank shares halted by NYSE after #b...</td>\n",
       "      <td>['bitcoin']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ğŸ˜ BITCOINLIVE is a Dutch platform aimed at inf...</td>\n",
       "      <td>2019-10-17 20:12:10</td>\n",
       "      <td>6769.0</td>\n",
       "      <td>1532</td>\n",
       "      <td>25483</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:58:48</td>\n",
       "      <td>ğŸ˜ Today, that's this #Thursday, we will do a \"...</td>\n",
       "      <td>['Thursday', 'Btc', 'wallet', 'security']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tdlmatias</td>\n",
       "      <td>London, England</td>\n",
       "      <td>IM Academy : The best #forex, #SelfEducation, ...</td>\n",
       "      <td>2014-11-10 10:50:37</td>\n",
       "      <td>128.0</td>\n",
       "      <td>332</td>\n",
       "      <td>924</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:48</td>\n",
       "      <td>Guys evening, I have read this article about B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crypto is the future</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I will post a lot of buying signals for BTC tr...</td>\n",
       "      <td>2019-09-28 16:48:12</td>\n",
       "      <td>625.0</td>\n",
       "      <td>129</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:33</td>\n",
       "      <td>$BTC A big chance in a billion! Price: \\487264...</td>\n",
       "      <td>['Bitcoin', 'FX', 'BTC', 'crypto']</td>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alex Kirchmaier ğŸ‡¦ğŸ‡¹ğŸ‡¸ğŸ‡ª #FactsSuperspreader</td>\n",
       "      <td>Europa</td>\n",
       "      <td>Co-founder @RENJERJerky | Forbes 30Under30 | I...</td>\n",
       "      <td>2016-02-03 13:15:55</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>1472</td>\n",
       "      <td>10482</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:06</td>\n",
       "      <td>This network is secured by 9 508 nodes as of t...</td>\n",
       "      <td>['BTC']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_name    user_location  \\\n",
       "0                             DeSota Wilson      Atlanta, GA   \n",
       "1                                  CryptoND              NaN   \n",
       "2                                 Tdlmatias  London, England   \n",
       "3                      Crypto is the future              NaN   \n",
       "4  Alex Kirchmaier ğŸ‡¦ğŸ‡¹ğŸ‡¸ğŸ‡ª #FactsSuperspreader           Europa   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Biz Consultant, real estate, fintech, startups...  2009-04-26 20:05:09   \n",
       "1  ğŸ˜ BITCOINLIVE is a Dutch platform aimed at inf...  2019-10-17 20:12:10   \n",
       "2  IM Academy : The best #forex, #SelfEducation, ...  2014-11-10 10:50:37   \n",
       "3  I will post a lot of buying signals for BTC tr...  2019-09-28 16:48:12   \n",
       "4  Co-founder @RENJERJerky | Forbes 30Under30 | I...  2016-02-03 13:15:55   \n",
       "\n",
       "   user_followers user_friends user_favourites user_verified  \\\n",
       "0          8534.0         7605            4838         False   \n",
       "1          6769.0         1532           25483         False   \n",
       "2           128.0          332             924         False   \n",
       "3           625.0          129              14         False   \n",
       "4          1249.0         1472           10482         False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2021-02-10 23:59:04  Blue Ridge Bank shares halted by NYSE after #b...   \n",
       "1  2021-02-10 23:58:48  ğŸ˜ Today, that's this #Thursday, we will do a \"...   \n",
       "2  2021-02-10 23:54:48  Guys evening, I have read this article about B...   \n",
       "3  2021-02-10 23:54:33  $BTC A big chance in a billion! Price: \\487264...   \n",
       "4  2021-02-10 23:54:06  This network is secured by 9 508 nodes as of t...   \n",
       "\n",
       "                                    hashtags               source is_retweet  \n",
       "0                                ['bitcoin']      Twitter Web App      False  \n",
       "1  ['Thursday', 'Btc', 'wallet', 'security']  Twitter for Android      False  \n",
       "2                                        NaN      Twitter Web App      False  \n",
       "3         ['Bitcoin', 'FX', 'BTC', 'crypto']              dlvr.it      False  \n",
       "4                                    ['BTC']      Twitter Web App      False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ceb29",
   "metadata": {},
   "source": [
    "### 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d866e9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.751629e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.907116e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.109416e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.030000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.297000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.359728e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_followers\n",
       "count    1.751629e+06\n",
       "mean     8.907116e+03\n",
       "std      1.109416e+05\n",
       "min      0.000000e+00\n",
       "25%      4.300000e+01\n",
       "50%      2.030000e+02\n",
       "75%      1.297000e+03\n",
       "max      2.359728e+07"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "# ì“¸ë°ëŠ” ì—†ì–´ë³´ì„ êµìˆ˜ë‹˜ ìë£Œì— ìˆì–´ì„œ ë„£ì–´ë´„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faefd8b",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd063fb3",
   "metadata": {},
   "source": [
    "### Cecking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fa432be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name               27\n",
       "user_location       843314\n",
       "user_description    206838\n",
       "user_created            26\n",
       "user_followers          46\n",
       "user_friends            46\n",
       "user_favourites         46\n",
       "user_verified           46\n",
       "date                    46\n",
       "text                    46\n",
       "hashtags             17003\n",
       "source                3573\n",
       "is_retweet             259\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # nullê°’ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bea572",
   "metadata": {},
   "source": [
    "### 4. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "176edfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re # ì •ê·œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7fe55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "506542eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x24c2bd754f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c711ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29f0a54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-e8e9d82e3e95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^a-zA-Z]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m## ì˜ì–´ ë¬¸ì¥ì— ê°ì£¼ ë“±ê³¼ ê°™ì€ ì´ìœ ë¡œ íŠ¹ìˆ˜ ë¬¸ìê°€ ì„ì—¬ìˆìŠµë‹ˆë‹¤.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m## ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì‹¶ë‹¤ë©´ ì•ŒíŒŒë²³ ì™¸ì˜ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë“±ì˜ ì‚¬ìš© ìš©ë„ë¡œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review'"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "corpus = []\n",
    "for i in range(0,1000):\n",
    "    review = re.sub('[^a-zA-Z]',' ',df['Review'][i]) \n",
    "    ## ì˜ì–´ ë¬¸ì¥ì— ê°ì£¼ ë“±ê³¼ ê°™ì€ ì´ìœ ë¡œ íŠ¹ìˆ˜ ë¬¸ìê°€ ì„ì—¬ìˆìŠµë‹ˆë‹¤. \n",
    "    ## ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì‹¶ë‹¤ë©´ ì•ŒíŒŒë²³ ì™¸ì˜ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë“±ì˜ ì‚¬ìš© ìš©ë„ë¡œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    review = review.lower() ## ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¾¸ì–´ ì¤ë‹ˆë‹¤.\n",
    "    review = review.split()\n",
    "    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    #stemmer ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ì—ì„œ ì ‘ì‚¬ë¥¼ ë¹¼ê³  review ì— ë„£ì–´ì¤ë‹ˆë‹¤. \n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    #appendë¥¼ ì‚¬ìš©í•´ì„œ ê³„ì† ë¶™ì—¬ ë‚˜ê°‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782834dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92812a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue Ridge Bank shares halted by NYSE after #b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ˜ Today, that's this #Thursday, we will do a \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$BTC A big chance in a billion! Price: \\487264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This network is secured by 9 508 nodes as of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ğŸ’¹ Trade #Crypto on #Binance \\n\\nğŸ“Œ Enjoy #Cashb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets\n",
       "0  Blue Ridge Bank shares halted by NYSE after #b...\n",
       "1  ğŸ˜ Today, that's this #Thursday, we will do a \"...\n",
       "3  $BTC A big chance in a billion! Price: \\487264...\n",
       "4  This network is secured by 9 508 nodes as of t...\n",
       "5  ğŸ’¹ Trade #Crypto on #Binance \\n\\nğŸ“Œ Enjoy #Cashb..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['hashtags'], inplace=True)\n",
    "\n",
    "df = df[['text']] \n",
    "df.columns = ['tweets']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed27091c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lily0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lily0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lily0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = nltk.corpus.stopwords.words(['english'])\n",
    "\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6a5935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def cleaning(data):\n",
    "  #remove urls\n",
    "  tweet_without_url = re.sub(r'http\\S+',' ', data)\n",
    "\n",
    "  #remove hashtags\n",
    "  tweet_without_hashtag = re.sub(r'#\\w+', ' ', tweet_without_url)\n",
    "\n",
    "  #3. Remove mentions and characters that not in the English alphabets\n",
    "  tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n",
    "  precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n",
    "\n",
    "    #2. Tokenize\n",
    "  tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n",
    "    \n",
    "    #3. Remove Puncs\n",
    "  tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n",
    "    \n",
    "    #4. Removing Stopwords\n",
    "  tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #5. lemma\n",
    "  text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #6. Joining\n",
    "  return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ab8f0",
   "metadata": {},
   "source": [
    "# ì—¬ê¸°ë¶€í„° ì‹¤í–‰ ì•ˆë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e1bda71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweets'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-3fe32902da5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_tweets'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweets'"
     ]
    }
   ],
   "source": [
    "df['cleaned_tweets'] = df['tweets'].apply(cleaning)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "164871a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(tweet):\n",
    "  return TextBlob(tweet).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(tweet):\n",
    "  return TextBlob(tweet).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60a04b7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cleaned_tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cleaned_tweets'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-51ba87445676>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subjectivity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetSubjectivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'polarity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetPolarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cleaned_tweets'"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df['cleaned_tweets'].apply(getSubjectivity)\n",
    "df['polarity'] = df['cleaned_tweets'].apply(getPolarity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(score):\n",
    "  if score < 0:\n",
    "    return 'negative'\n",
    "  elif score == 0:\n",
    "    return 'neutral'\n",
    "  else:\n",
    "    return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['polarity'].apply(getSentiment)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73012781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.layers as Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Categorical Variable\n",
    "X = df['cleaned_tweets']\n",
    "y = pd.get_dummies(df['sentiment']).values\n",
    "num_classes = df['sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 101 # fix random seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=seed)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 30\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "max_features = 20000\n",
    "embed_dim = 100\n",
    "\n",
    "np.random.seed(seed)\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))    \n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c786b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                          epochs=epochs, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class with test set\n",
    "y_pred_test =  np.argmax(model.predict(X_test), axis=1)\n",
    "print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis=1),y_pred_test)*100))\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "confmat = confusion_matrix(np.argmax(y_test,axis=1), y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "  for j in range(confmat.shape[1]):\n",
    "    ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "  plt.xlabel('Predicted label')\n",
    "  plt.ylabel('True label')\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad27297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
